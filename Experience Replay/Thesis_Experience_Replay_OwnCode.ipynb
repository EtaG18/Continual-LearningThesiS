{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMB52PDjeCC1j6t1mbgUjL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"57b5M_HQKYyz"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from backbone import xavier, num_flat_features\n","\n","class MNISTMLP(nn.Module):\n","    def __init__(self, input_size: int, output_size: int, hidden_size=100) -> None:\n","        super(MNISTMLP, self).__init__()\n","        self.input_size = input_size\n","        self.output_size = output_size\n","        self.fc1 = nn.Linear(self.input_size, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self._features = nn.Sequential(\n","            self.fc1,\n","            nn.ReLU(),\n","            self.fc2,\n","            nn.ReLU(),\n","        )\n","        self._classifier = nn.Linear(hidden_size, self.output_size)\n","        self.net = nn.Sequential(self._features, self._classifier)\n","        self.reset_parameters()\n","\n","    def features(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Returns the non-activated output of the second-last layer.\n","        :param x: input tensor (batch_size, input_size)\n","        :return: output tensor (100)\n","        \"\"\"\n","        x = x.view(-1, num_flat_features(x))\n","        return self._features(x)\n","\n","    def classifier(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Returns the non-activated output of the second-last layer.\n","        :param x: input tensor (batch_size, input_size)\n","        :return: output tensor (100)\n","        \"\"\"\n","        return self._classifier(x)\n","\n","    def reset_parameters(self) -> None:\n","        \"\"\"\n","        Calls the Xavier parameter initialization function.\n","        \"\"\"\n","        self.net.apply(xavier)\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"\n","        Compute a forward pass.\n","        :param x: input tensor (batch_size, input_size)\n","        :return: output tensor (output_size)\n","        \"\"\"\n","        x = x.view(-1, num_flat_features(x))\n","        return self.net(x)\n","\n","    def get_params(self) -> torch.Tensor:\n","        \"\"\"\n","        Returns all the parameters concatenated in a single tensor.\n","        :return: parameters tensor (input_size * 100 + 100 + 100 * 100 + 100 +\n","                                    + 100 * output_size + output_size)\n","        \"\"\"\n","        params = []\n","        for pp in list(self.parameters()):\n","            params.append(pp.view(-1))\n","        return torch.cat(params)\n","\n","    def set_params(self, new_params: torch.Tensor) -> None:\n","        \"\"\"\n","        Sets the parameters to a given value.\n","        :param new_params: concatenated values to be set (input_size * 100\n","                    + 100 + 100 * 100 + 100 + 100 * output_size + output_size)\n","        \"\"\"\n","        assert new_params.size() == self.get_params().size()\n","        progress = 0\n","        for pp in list(self.parameters()):\n","            cand_params = new_params[progress: progress +\n","                torch.tensor(pp.size()).prod()].view(pp.size())\n","            progress += torch.tensor(pp.size()).prod()\n","            pp.data = cand_params"]},{"cell_type":"code","source":["from torch.utils.data import DataLoader\n","from torchvision.datasets import CIFAR10\n","import torchvision.transforms as transforms\n","from backbone.ResNet import ResNet18\n","import torch.nn.functional as F\n","from utils.conf import base_path\n","from PIL import Image\n","from datasets.utils.validation import get_train_val\n","from datasets.utils.continual_dataset import ContinualDataset, store_masked_loaders\n","from datasets.utils.continual_dataset import get_previous_train_loader\n","from datasets.transforms.denormalization import DeNormalize\n","import torch\n","from typing import Tuple\n","import numpy as np\n","\n","class MyCIFAR10(CIFAR10):\n","    \"\"\"\n","    Overrides the CIFAR10 dataset to change the getitem function.\n","    \"\"\"\n","    def __init__(self, root, train=True, transform=None,\n","                 target_transform=None, download=False) -> None:\n","        self.not_aug_transform = transforms.Compose([transforms.ToTensor()])\n","        super(MyCIFAR10, self).__init__(root, train, transform, target_transform, download)\n","\n","    def __getitem__(self, index: int) -> Tuple[type(Image), int, type(Image)]:\n","        img, target = self.data[index], self.targets[index]\n","        img = Image.fromarray(img, mode='RGB')\n","        original_img = img.copy()\n","        not_aug_img = self.not_aug_transform(original_img)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","        if hasattr(self, 'logits'):\n","            return img, target, not_aug_img, self.logits[index]\n","        return img, target, not_aug_img\n","\n","\n","class SequentialCIFAR10(ContinualDataset):\n","    NAME = 'seq-cifar10'\n","    SETTING = 'class-il'\n","    N_CLASSES_PER_TASK = 2\n","    N_TASKS = 5\n","    TRANSFORM = transforms.Compose(\n","            [transforms.RandomCrop(32, padding=4),\n","             transforms.RandomHorizontalFlip(),\n","             transforms.ToTensor(),\n","             transforms.Normalize((0.4914, 0.4822, 0.4465),\n","                                  (0.2470, 0.2435, 0.2615))])\n","    def get_data_loaders(self, nomask=False):\n","        transform = self.TRANSFORM\n","        test_transform = transforms.Compose(\n","            [transforms.ToTensor(), self.get_normalization_transform()])\n","        train_dataset = MyCIFAR10(base_path() + 'CIFAR10', train=True,download=True, transform=transform)\n","        if self.args.validation:\n","            train_dataset, test_dataset = get_train_val(train_dataset,test_transform, self.NAME)\n","        else:\n","            test_dataset = CIFAR10(base_path() + 'CIFAR10', train=False,download=True, transform=test_transform)\n","\n","        if not nomask:\n","            if isinstance(train_dataset.targets, list):\n","                train_dataset.targets = torch.tensor(train_dataset.targets, dtype=torch.long)\n","            if isinstance(test_dataset.targets, list):\n","                test_dataset.targets = torch.tensor(test_dataset.targets, dtype=torch.long)\n","            train, test = store_masked_loaders(train_dataset, test_dataset, self)\n","            return train, test\n","        else:\n","            train_loader = DataLoader(train_dataset,batch_size=32, shuffle=True, num_workers=2)\n","            test_loader = DataLoader(test_dataset,batch_size=32, shuffle=False, num_workers=2)\n","            return train_loader, test_loader\n","\n","    def get_joint_loaders(self, nomask=False):\n","        return self.get_data_loaders(nomask=True)\n","\n","    def not_aug_dataloader(self, args, batch_size):\n","        if hasattr(args, 'iba') and args.iba:\n","            transform = transforms.Compose([transforms.ToTensor()])\n","        else:\n","            transform = transforms.Compose([transforms.ToTensor(),self.get_normalization_transform()])\n","        train_dataset = MyCIFAR10(base_path() + 'CIFAR10', train=True,download=True, transform=transform)\n","\n","        if isinstance(train_dataset.targets, list):\n","            train_dataset.targets = torch.tensor(train_dataset.targets, dtype=torch.long)\n","\n","        train_mask = np.logical_and(np.array(train_dataset.targets) >= (self.i - 1) * self.N_CLASSES_PER_TASK,np.array(train_dataset.targets) < self.i * self.N_CLASSES_PER_TASK)\n","        train_dataset.data = train_dataset.data[train_mask]\n","        train_dataset.targets = np.array(train_dataset.targets)[train_mask]\n","        train_loader = get_previous_train_loader(train_dataset, batch_size, self)\n","        return train_loader\n","\n","\n","    def get_transform():\n","        transform = transforms.Compose(\n","            [transforms.ToPILImage(), SequentialCIFAR10.TRANSFORM])\n","        return transform\n","\n","    def get_backbone():\n","        return ResNet18(SequentialCIFAR10.N_CLASSES_PER_TASK * SequentialCIFAR10.N_TASKS)\n","\n","    def get_loss():\n","        return F.cross_entropy\n","\n","    def get_normalization_transform():\n","        transform = transforms.Normalize((0.4914, 0.4822, 0.4465),(0.2470, 0.2435, 0.2615))\n","        return transform"],"metadata":{"id":"7EnYgsXHO3mz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Er(ContinualModel):\n","    NAME = 'er'\n","    COMPATIBILITY = ['class-il', 'domain-il', 'task-il', 'general-continual']\n","\n","    def __init__(self, backbone, loss, args, transform):\n","        super(Er, self).__init__(backbone, loss, args, transform)\n","        self.buffer = Buffer(self.args.buffer_size, self.device)\n","        self.transform = None\n","\n","    def observe(self, inputs, labels, not_aug_inputs):\n","        real_batch_size = inputs.shape[0]\n","        self.opt.zero_grad()\n","        if not self.buffer.is_empty():\n","            buf_inputs, buf_labels = self.buffer.get_data(\n","                self.args.minibatch_size, transform=self.transform)\n","            inputs = torch.cat((inputs, buf_inputs))\n","            labels = torch.cat((labels, buf_labels))\n","        outputs = self.net(inputs)\n","        loss = self.loss(outputs, labels)\n","        loss.backward()\n","        self.opt.step()\n","        self.buffer.add_data(examples=inputs[:real_batch_size],labels=labels[:real_batch_size])\n","        return loss.item()"],"metadata":{"id":"0Qeo85dmO90V"},"execution_count":null,"outputs":[]}]}