{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/DQX1bRb/+3fuySjToTBq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Yk5BbTFHQsE0","executionInfo":{"status":"ok","timestamp":1709205660006,"user_tz":-330,"elapsed":10990,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}}},"outputs":[],"source":["import torch\n","torch.cuda.is_available()\n","import torch\n","import torch.nn as nn\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import numpy as np\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","import keras\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import Subset, DataLoader, ConcatDataset\n","import random\n","import numpy as np\n","batch_size=8\n","from sklearn.metrics import precision_score, recall_score, f1_score"]},{"cell_type":"code","source":["# Define the model architecture\n","class CNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.classifier = nn.Linear(3136, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","# Define the iCaRL class\n","class iCaRL:\n","    def __init__(self, device, num_classes, batch_size, memory_size):\n","        self.device = device\n","        self.num_classes = num_classes\n","        self.batch_size = batch_size\n","        self.memory_size = memory_size\n","        self.model = CNN(num_classes).to(device)\n","        self.exemplar_sets = []\n","\n","    def train(self, train_dataset, lr, num_epochs):\n","        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-5)\n","        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49, 63], gamma=0.2)\n","\n","        for epoch in range(num_epochs):\n","            self.model.train()\n","            running_loss = 0.0\n","            for inputs, labels in train_loader:\n","                inputs, labels = inputs.to(self.device), labels.to(self.device)\n","                optimizer.zero_grad()\n","                outputs = self.model(inputs)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","                running_loss += loss.item()\n","            scheduler.step()\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n","\n","    def test(self, test_dataset, lr, num_epochs):\n","        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=True)\n","        criterion = nn.CrossEntropyLoss()\n","\n","        for epoch in range(num_epochs):\n","            self.model.train()\n","            running_loss = 0.0\n","            test_loss = 0\n","            correct = 0\n","            all_preds = []\n","            all_labels = []\n","            batch_size = 1\n","            for inputs, labels in test_loader:\n","                inputs, labels = inputs.to(self.device), labels.to(self.device)\n","                outputs = self.model(inputs)\n","                loss = criterion(outputs, labels)\n","                pred = outputs.max(1)[1]\n","                correct += pred.eq(labels).sum().item()\n","                preds = pred.cpu().numpy()\n","                preds = torch.from_numpy(preds).float().to(device)\n","                all_preds.extend(preds)\n","                all_labels.extend(labels)\n","                running_loss += loss.item()\n","                test_loss = running_loss / len(test_dataset)\n","            # Calculate metrics\n","            precision = precision_score(all_labels, all_preds, average='weighted')\n","            recall = recall_score(all_labels, all_preds, average='weighted')\n","            f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","            print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(test_loss, correct, len(test_dataset), 100. * correct / len(test_dataset)))\n","            print('Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\\n'.format(precision, recall, f1))\n","\n","            return 100. * correct / len(labels)\n","\n","\n","    def construct_exemplar_set(self, dataset, m):\n","        exemplar_set = []\n","        class_means = []\n","\n","        for class_idx in range(self.num_classes):\n","            class_indices = [idx for idx in dataset.indices if dataset.dataset.targets[idx] == class_idx]\n","            random.shuffle(class_indices)\n","            class_indices = class_indices[:m]\n","\n","            features = []\n","            for idx in class_indices:\n","                img, _ = dataset.dataset[idx]  # Access data and label from the original dataset\n","                img = img.numpy().astype(np.float32) / 255.0\n","                img = torch.FloatTensor(img).unsqueeze(0).to(self.device)\n","                # print(img.shape)\n","                feature = self.model.features(img).cpu().squeeze().detach().numpy()\n","                features.append(feature)\n","            features = np.array(features)\n","            class_mean = np.mean(features, axis=0)\n","\n","            exemplar_set.append(class_mean / np.linalg.norm(class_mean))\n","            class_means.append(class_mean / np.linalg.norm(class_mean))\n","\n","        self.exemplar_sets.append(exemplar_set)\n","        self.class_means = class_means\n","\n","    def reduce_exemplar_set(self, m):\n","        for exemplar_set in self.exemplar_sets:\n","            exemplar_set = exemplar_set[:m]"],"metadata":{"id":"Z-qzO33HQwlB","executionInfo":{"status":"ok","timestamp":1709205660007,"user_tz":-330,"elapsed":36,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define hyperparameters\n","num_classes = 10\n","batch_size = 8\n","memory_size = 2000\n","lr = 0.0005\n","num_epochs = 20\n","m = memory_size // num_classes\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","# Load CIFAR-10 dataset\n","train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","\n","# Initialize iCaRL\n","icarl = iCaRL(device, num_classes, batch_size, memory_size)\n","\n","\n","# Define the number of classes to train on for each task\n","classes_per_task = 5  # You can set this to any number you prefer\n","\n","# Partition the dataset into subsets of classes\n","num_tasks = (num_classes + classes_per_task - 1) // classes_per_task\n","train_datasets = []\n","for task_idx in range(num_tasks):\n","    start_class_idx = task_idx * classes_per_task\n","    end_class_idx = min(start_class_idx + classes_per_task, num_classes)\n","    indices = [idx for idx, label in enumerate(train_dataset.targets)\n","               if start_class_idx <= label < end_class_idx]\n","    train_datasets.append(Subset(train_dataset, indices))\n","\n","# Incremental training\n","for i, train_dataset in enumerate(train_datasets):\n","    print(f'Training on task {i + 1}')\n","    icarl.train(train_dataset, lr, num_epochs)\n","    print(f'Constructing exemplar set for task {i + 1}')\n","    icarl.construct_exemplar_set(train_dataset, m)\n","    if i > 0:\n","        print(f'Reducing exemplar set for task {i}')\n","        icarl.reduce_exemplar_set(m)\n","\n","# Evaluation\n","with torch.no_grad():\n","  icarl.test(test_dataset, lr, num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X6q02yL95vbi","executionInfo":{"status":"ok","timestamp":1709207422740,"user_tz":-330,"elapsed":1762766,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}},"outputId":"9b950c7f-a646-4975-c75f-61221c2ed882"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 9912422/9912422 [00:00<00:00, 91025292.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 28881/28881 [00:00<00:00, 31504731.81it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 1648877/1648877 [00:00<00:00, 27014778.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 4542/4542 [00:00<00:00, 13956431.33it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n","\n","Training on task 1\n","Epoch [1/20], Loss: 0.1655\n","Epoch [2/20], Loss: 0.0514\n","Epoch [3/20], Loss: 0.0355\n","Epoch [4/20], Loss: 0.0286\n","Epoch [5/20], Loss: 0.0241\n","Epoch [6/20], Loss: 0.0213\n","Epoch [7/20], Loss: 0.0190\n","Epoch [8/20], Loss: 0.0172\n","Epoch [9/20], Loss: 0.0158\n","Epoch [10/20], Loss: 0.0146\n","Epoch [11/20], Loss: 0.0133\n","Epoch [12/20], Loss: 0.0125\n","Epoch [13/20], Loss: 0.0116\n","Epoch [14/20], Loss: 0.0108\n","Epoch [15/20], Loss: 0.0099\n","Epoch [16/20], Loss: 0.0095\n","Epoch [17/20], Loss: 0.0087\n","Epoch [18/20], Loss: 0.0085\n","Epoch [19/20], Loss: 0.0077\n","Epoch [20/20], Loss: 0.0071\n","Constructing exemplar set for task 1\n","Training on task 2\n","Epoch [1/20], Loss: 0.1141\n","Epoch [2/20], Loss: 0.0439\n","Epoch [3/20], Loss: 0.0347\n","Epoch [4/20], Loss: 0.0287\n","Epoch [5/20], Loss: 0.0257\n","Epoch [6/20], Loss: 0.0228\n","Epoch [7/20], Loss: 0.0204\n","Epoch [8/20], Loss: 0.0182\n","Epoch [9/20], Loss: 0.0171\n","Epoch [10/20], Loss: 0.0156\n","Epoch [11/20], Loss: 0.0145\n","Epoch [12/20], Loss: 0.0133\n","Epoch [13/20], Loss: 0.0126\n","Epoch [14/20], Loss: 0.0115\n","Epoch [15/20], Loss: 0.0102\n","Epoch [16/20], Loss: 0.0097\n","Epoch [17/20], Loss: 0.0092\n","Epoch [18/20], Loss: 0.0087\n","Epoch [19/20], Loss: 0.0078\n","Epoch [20/20], Loss: 0.0069\n","Constructing exemplar set for task 2\n","Reducing exemplar set for task 1\n","Test set: Average loss: 0.8934, Accuracy: 4841/10000 (48%)\n","Precision: 0.4584, Recall: 0.4841, F1 Score: 0.3216\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ASnYYooW5_yD","executionInfo":{"status":"ok","timestamp":1709207422741,"user_tz":-330,"elapsed":13,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}}},"execution_count":3,"outputs":[]}]}