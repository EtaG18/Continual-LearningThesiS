{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12615,"status":"ok","timestamp":1708938815249,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"},"user_tz":-330},"id":"DrNhRcYKKoUv","outputId":"630d12f2-39d1-4c66-d2ba-83607d0d2a41"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.3.1)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (23.2)\n","Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu121)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.10.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.2.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n"]}],"source":[" !pip install -U torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g06hCQe3fYud"},"outputs":[],"source":["import torch\n","torch.cuda.is_available()\n","import torch\n","import torch.nn as nn\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import numpy as np\n","from tqdm.auto import tqdm\n","import matplotlib.pyplot as plt\n","import keras\n","import os\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import Subset, DataLoader, ConcatDataset\n","import random\n","import numpy as np\n","batch_size=8\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","import torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QRrkAgAhHbQp"},"outputs":[],"source":["from torchmetrics import Precision\n","# Define the model architecture\n","class CNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CNN, self).__init__()\n","        self.features = nn.Sequential(\n","            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2),\n","            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","        self.classifier = nn.Linear(64*8*8, num_classes)\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","# Define the iCaRL class\n","class iCaRL:\n","    def __init__(self, device, num_classes, batch_size, memory_size):\n","        self.device = device\n","        self.num_classes = num_classes\n","        self.batch_size = batch_size\n","        self.memory_size = memory_size\n","        self.model = CNN(num_classes).to(device)\n","        self.exemplar_sets = []\n","\n","    def train(self, train_dataset, lr, num_epochs):\n","        train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = optim.SGD(self.model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-5)\n","        scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[49, 63], gamma=0.2)\n","\n","        for epoch in range(num_epochs):\n","            self.model.train()\n","            running_loss = 0.0\n","            for inputs, labels in train_loader:\n","                inputs, labels = inputs.to(self.device), labels.to(self.device)\n","                optimizer.zero_grad()\n","                outputs = self.model(inputs)\n","                loss = criterion(outputs, labels)\n","                loss.backward()\n","                optimizer.step()\n","                running_loss += loss.item()\n","            scheduler.step()\n","            print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n","\n","    def test(self, test_dataset, lr, num_epochs):\n","        test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=True)\n","        criterion = nn.CrossEntropyLoss()\n","        # precision = Precision(average='weighted')\n","        for epoch in range(num_epochs):\n","            self.model.train()\n","            running_loss = 0.0\n","            test_loss = 0\n","            correct = 0\n","            all_preds = []\n","            all_labels = []\n","            batch_size = 1\n","            for inputs, labels in test_loader:\n","                inputs, labels = inputs.to(self.device), labels.to(self.device)\n","                outputs = self.model(inputs).to(self.device)\n","                loss = criterion(outputs, labels)\n","                pred = outputs.max(1)[1].to(self.device)\n","                correct += pred.eq(labels).sum().item()\n","                pred = pred.cpu().numpy()\n","                all_preds.extend(pred)\n","                all_labels.extend(labels)\n","                running_loss += loss.item()\n","                test_loss = running_loss / len(test_dataset)\n","\n","            # Calculate metrics\n","            precision = precision_score(all_labels, all_preds, average='weighted')\n","            recall = recall_score(all_labels, all_preds, average='weighted')\n","            f1 = f1_score(all_labels, all_preds, average='weighted')\n","\n","            print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)'.format(test_loss, correct, len(test_dataset), 100. * correct / len(test_dataset)))\n","            print('Precision: {:.4f}, Recall: {:.4f}, F1 Score: {:.4f}\\n'.format(precision, recall, f1))\n","\n","            return 100. * correct / len(labels)\n","\n","\n","    def construct_exemplar_set(self, dataset, m):\n","        exemplar_set = []\n","        class_means = []\n","\n","        for class_idx in range(self.num_classes):\n","            class_indices = [idx for idx in dataset.indices if dataset.dataset.targets[idx] == class_idx]\n","            random.shuffle(class_indices)\n","            class_indices = class_indices[:m]\n","\n","            features = []\n","            for idx in class_indices:\n","                img, _ = dataset.dataset[idx]  # Access data and label from the original dataset\n","                img = img.numpy().astype(np.float32) / 255.0\n","                img = torch.FloatTensor(img).unsqueeze(0).to(self.device)\n","                # print(img.shape)\n","                feature = self.model.features(img).squeeze().cpu().detach().numpy()\n","                features.append(feature)\n","            features = np.array(features)\n","            class_mean = np.mean(features, axis=0)\n","\n","            exemplar_set.append(class_mean / np.linalg.norm(class_mean))\n","            class_means.append(class_mean / np.linalg.norm(class_mean))\n","\n","        self.exemplar_sets.append(exemplar_set)\n","        self.class_means = class_means\n","\n","    def reduce_exemplar_set(self, m):\n","        for exemplar_set in self.exemplar_sets:\n","            exemplar_set = exemplar_set[:m]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0m2AMQDs9_2A","executionInfo":{"status":"ok","timestamp":1708940427914,"user_tz":-330,"elapsed":778405,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}},"outputId":"f4492779-9aba-4fb6-9840-df8d5a813e3f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","Training on task 1\n","Epoch [1/20], Loss: 1.6443\n","Epoch [2/20], Loss: 1.3498\n","Epoch [3/20], Loss: 1.2510\n","Epoch [4/20], Loss: 1.1953\n","Epoch [5/20], Loss: 1.1490\n","Epoch [6/20], Loss: 1.1053\n","Epoch [7/20], Loss: 1.0663\n","Epoch [8/20], Loss: 1.0291\n","Epoch [9/20], Loss: 1.0000\n","Epoch [10/20], Loss: 0.9747\n","Epoch [11/20], Loss: 0.9560\n","Epoch [12/20], Loss: 0.9373\n","Epoch [13/20], Loss: 0.9208\n","Epoch [14/20], Loss: 0.9042\n","Epoch [15/20], Loss: 0.8912\n","Epoch [16/20], Loss: 0.8788\n","Epoch [17/20], Loss: 0.8661\n","Epoch [18/20], Loss: 0.8538\n","Epoch [19/20], Loss: 0.8466\n","Epoch [20/20], Loss: 0.8331\n","Constructing exemplar set for task 1\n","Training on task 2\n","Epoch [1/20], Loss: 1.6901\n","Epoch [2/20], Loss: 0.9360\n","Epoch [3/20], Loss: 0.8455\n","Epoch [4/20], Loss: 0.7922\n","Epoch [5/20], Loss: 0.7526\n","Epoch [6/20], Loss: 0.7224\n","Epoch [7/20], Loss: 0.6979\n","Epoch [8/20], Loss: 0.6749\n","Epoch [9/20], Loss: 0.6565\n","Epoch [10/20], Loss: 0.6426\n","Epoch [11/20], Loss: 0.6277\n","Epoch [12/20], Loss: 0.6164\n","Epoch [13/20], Loss: 0.6064\n","Epoch [14/20], Loss: 0.5971\n","Epoch [15/20], Loss: 0.5884\n","Epoch [16/20], Loss: 0.5796\n","Epoch [17/20], Loss: 0.5707\n","Epoch [18/20], Loss: 0.5635\n","Epoch [19/20], Loss: 0.5583\n","Epoch [20/20], Loss: 0.5528\n","Constructing exemplar set for task 2\n","Reducing exemplar set for task 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["Test set: Average loss: 0.0603, Accuracy: 3950/10000 (40%)\n","Precision: 0.2002, Recall: 0.3950, F1 Score: 0.2649\n","\n"]}],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","# Set device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Define hyperparameters\n","num_classes = 10\n","batch_size = 8\n","memory_size = 2000\n","lr = 0.0005\n","num_epochs = 20\n","m = memory_size // num_classes\n","\n","# Define transforms\n","transform = transforms.Compose([\n","    transforms.ToTensor()\n","])\n","\n","# Load CIFAR-10 dataset\n","train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n","test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n","\n","# Initialize iCaRL\n","icarl = iCaRL(device, num_classes, batch_size, memory_size)\n","\n","# Define the number of classes to train on for each task\n","classes_per_task = 5  # You can set this to any number you prefer\n","\n","# Partition the dataset into subsets of classes\n","num_tasks = (num_classes + classes_per_task - 1) // classes_per_task\n","train_datasets = []\n","for task_idx in range(num_tasks):\n","    start_class_idx = task_idx * classes_per_task\n","    end_class_idx = min(start_class_idx + classes_per_task, num_classes)\n","    indices = [idx for idx, label in enumerate(train_dataset.targets)\n","               if start_class_idx <= label < end_class_idx]\n","    train_datasets.append(Subset(train_dataset, indices))\n","\n","# Incremental training\n","for i, train_dataset in enumerate(train_datasets):\n","    print(f'Training on task {i + 1}')\n","    icarl.train(train_dataset, lr, num_epochs)\n","    print(f'Constructing exemplar set for task {i + 1}')\n","    icarl.construct_exemplar_set(train_dataset, m)\n","    if i > 0:\n","        print(f'Reducing exemplar set for task {i}')\n","        icarl.reduce_exemplar_set(m)\n","\n","# Evaluation\n","with torch.no_grad():\n","  icarl.test(test_dataset, lr, num_epochs)"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1Ewpx75pqKGR31NUsOZ4hWFG0Pj9Az86v","authorship_tag":"ABX9TyMYheByRHnW26yrH9x+jwEd"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}