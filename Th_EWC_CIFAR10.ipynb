{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6vfytEQgmLnj","executionInfo":{"status":"ok","timestamp":1699605231554,"user_tz":-330,"elapsed":3883,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv2D\n","from tensorflow.keras.layers import MaxPooling2D\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras import layers\n","from tensorflow.keras import backend as K\n","import numpy as np\n","from tqdm import tqdm\n","\n","\n","def evaluate(model, test_x, test_y):\n","    acc = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n","    for imgs, labels in zip(test_x, test_y):\n","        preds = model.predict_on_batch(np.array([imgs]))\n","        acc.update_state(labels, preds)\n","    return round(100*acc.result().numpy(), 2)\n","\n","\n","def permute_task(train, test):\n","    train_shape, test_shape = train.shape, test.shape\n","    train_flat, test_flat = train.reshape((-1, 3072)), test.reshape((-1, 3072))\n","    idx = np.arange(train_flat.shape[1])\n","    np.random.shuffle(idx)\n","    train_permuted, test_permuted = train_flat[:, idx], test_flat[:, idx]\n","    return (train_permuted.reshape(train_shape), test_permuted.reshape(test_shape))\n","\n","\n","class Train:\n","\n","    def __init__(self, optimizer, loss_fn, prior_weights=None, lambda_=0.1):\n","        self.optimizer = optimizer\n","        self.loss_fn = loss_fn\n","        self.prior_weights = prior_weights\n","        self.lambda_ = lambda_\n","\n","    def train(self, model, epochs, train_task, fisher_matrix=None, test_tasks=None):\n","        # empty list to collect per epoch test acc of each task\n","        if test_tasks:\n","            test_acc = [[] for _ in test_tasks]\n","        else:\n","            test_acc = None\n","        for epoch in tqdm(range(epochs)):\n","            for batch in train_task:\n","                X, y = batch\n","                with tf.GradientTape() as tape:\n","                    pred = model(X)\n","                    loss = self.loss_fn(y, pred)\n","                    # if to execute training with EWC\n","                    if fisher_matrix is not None:\n","                        loss += self.compute_penalty_loss(model, fisher_matrix)\n","                grads = tape.gradient(loss, model.trainable_variables)\n","                self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","            # evaluate with the test set of task after each epoch\n","            if test_acc:\n","                for i in range(len(test_tasks)):\n","                    test_acc[i].append(evaluate(model, test_tasks[i][0], test_tasks[i][1]))\n","        print(test_acc)\n","        return test_acc\n","\n","    def compute_penalty_loss(self, model, fisher_matrix):\n","        penalty = 0.\n","        for u, v, w in zip(fisher_matrix, model.weights, self.prior_weights):\n","            penalty += tf.math.reduce_sum(u * tf.math.square(v - w))\n","        return 0.5 * self.lambda_ * penalty\n","\n","\n","class EWC:\n","\n","    def __init__(self, prior_model, data_samples, num_sample=30):\n","        self.prior_model = prior_model\n","        self.prior_weights = prior_model.weights\n","        self.num_sample = num_sample\n","        self.data_samples = data_samples\n","        self.fisher_matrix = self.compute_fisher()\n","\n","    def compute_fisher(self):\n","        weights = self.prior_weights\n","        fisher_accum = np.array([np.zeros(layer.numpy().shape) for layer in weights],\n","                           dtype=object\n","                          )\n","        for j in tqdm(range(self.num_sample)):\n","            idx = np.random.randint(self.data_samples.shape[0])\n","            with tf.GradientTape() as tape:\n","                logits = tf.nn.log_softmax(self.prior_model(np.array([self.data_samples[idx]])))\n","            grads = tape.gradient(logits, weights)\n","            for m in range(len(weights)):\n","                fisher_accum[m] += np.square(grads[m])\n","        fisher_accum /= self.num_sample\n","        return fisher_accum\n","\n","    def get_fisher(self):\n","        return self.fisher_matrix\n","\n","\n","from keras.models import Sequential\n","from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n","\n","class MLP3:\n","    def __init__(self, input_shape=(32,32,3), hidden_layers_neuron_list=[200, 100, 50, 25, 12], num_classes=10):\n","        self.input_shape = input_shape\n","        self.num_classes = num_classes\n","        self.hidden_layers_neuron_list = hidden_layers_neuron_list\n","        self.model = self.create_cnn()\n","\n","    def create_cnn(self):\n","        model = Sequential()\n","\n","        # Convolutional layers\n","        model.add(Conv2D(32, (3, 3), activation='relu', input_shape=self.input_shape))\n","        model.add(MaxPooling2D((2, 2),padding='same'))\n","        model.add(Conv2D(64, (3, 3), activation='relu'))\n","        model.add(MaxPooling2D((2, 2),padding='same'))\n","        model.add(Conv2D(128, (3, 3), activation='relu'))\n","        model.add(MaxPooling2D((2, 2),padding='same'))\n","        model.add(Conv2D(256, (3, 3), activation='relu'))\n","        model.add(MaxPooling2D((2, 2),padding='same'))\n","\n","        # Flatten layer\n","        model.add(Flatten())\n","\n","        # Dense layers\n","        model.add(Dense(self.hidden_layers_neuron_list[0], activation='relu'))\n","        model.add(Dense(self.hidden_layers_neuron_list[1], activation='relu'))\n","        model.add(Dense(self.hidden_layers_neuron_list[2], activation='relu'))\n","        model.add(Dense(self.hidden_layers_neuron_list[3], activation='relu'))\n","        model.add(Dense(self.hidden_layers_neuron_list[4], activation='relu'))\n","\n","        # Output layer\n","        model.add(Dense(self.num_classes, activation='softmax'))\n","\n","        return model\n","\n","    def get_uncompiled_model(self):\n","      return self.model\n","\n","    def get_compiled_model(self, optimizer, loss_fn, metrics ):\n","      compiled_model = self.model\n","      compiled_model.compile(optimizer, loss_fn, metrics)\n","      return compiled_model"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"YYxR9ZPkmbj-","executionInfo":{"status":"ok","timestamp":1699605231555,"user_tz":-330,"elapsed":11,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}}},"outputs":[],"source":["from tensorflow.keras.layers import Dense\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.models import Sequential\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"SvuLG3vEmdwh","executionInfo":{"status":"ok","timestamp":1699605233859,"user_tz":-330,"elapsed":2313,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}}},"outputs":[],"source":["epochs = 5\n","lambda_ = 0.01\n","lr = 0.0001\n","num_sample = 30\n","opt = tf.keras.optimizers.Adam(learning_rate=lr)\n","loss_fn=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23093,"status":"ok","timestamp":1699605256942,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"},"user_tz":-330},"id":"HZq-ti13mf3p","outputId":"c9aa5a42-f4a7-444a-9e0d-781bcbeb11f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 13s 0us/step\n"]}],"source":["(x_train_A, y_train_A), (x_test_A, y_test_A) = cifar10.load_data()\n","x_train_A = x_train_A.astype('float32')\n","x_test_A = x_test_A.astype('float32')\n","\n","train_A = tf.data.Dataset.from_tensor_slices((x_train_A, y_train_A)).shuffle(1000).batch(32)\n","test_A = (x_test_A, y_test_A)\n","\n","x_train_B, x_test_B = permute_task(x_train_A, x_test_A)\n","y_train_B, y_test_B = y_train_A, y_test_A\n","\n","train_B = tf.data.Dataset.from_tensor_slices((x_train_B, y_train_B)).shuffle(1000).batch(4)\n","test_B = (x_test_B, y_test_B)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWij0ywxmtgJ","outputId":"53720afb-7dfd-423f-fe5c-82912804ca67","executionInfo":{"status":"ok","timestamp":1699605936492,"user_tz":-330,"elapsed":679565,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/5 [00:00<?, ?it/s]WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7fd765f956c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7fd765f956c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","100%|██████████| 5/5 [11:19<00:00, 135.91s/it]"]},{"output_type":"stream","name":"stdout","text":["[[40.95, 51.1, 55.83, 60.5, 62.28]]\n","[INFO] Task A Original (SGD): 62.28\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["mlp = MLP3()\n","\n","trn_gd = Train(opt, loss_fn)\n","model = mlp.get_compiled_model(opt, loss_fn, ['accuracy'])\n","\n","acc_prior_A = trn_gd.train(model, epochs, train_A, test_tasks=[test_A])[0]\n","model.save('CIFAR10_A.h5')\n","print('[INFO] Task A Original (SGD): {}'.format(acc_prior_A[-1]))"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LvzsTtp3y2LP","outputId":"02364972-2aa1-4328-b9c9-35e1a4b8abfc","executionInfo":{"status":"ok","timestamp":1699605937409,"user_tz":-330,"elapsed":923,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 30/30 [00:00<00:00, 33.00it/s]\n"]}],"source":["# construct the fisher matrix using samples from task A\n","ewc = EWC(model, x_train_A, num_sample=num_sample)\n","f_matrix = ewc.get_fisher()"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Yesj6paTy25Q","executionInfo":{"status":"ok","timestamp":1699605937409,"user_tz":-330,"elapsed":3,"user":{"displayName":"Achyuth Kasyap","userId":"12117926900346930702"}}},"outputs":[],"source":["model_ewcB = mlp.get_compiled_model(opt, loss_fn, ['accuracy'])\n","model_ewcB.load_weights('CIFAR10_A.h5')\n","prior_weights = model_ewcB.get_weights()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kZsvMu1UvwDD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"489c6a3f-a097-481b-dbb8-6f8edf97c326"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 3/5 [53:33<35:38, 1069.13s/it]"]}],"source":["trn = Train(opt, loss_fn, prior_weights=prior_weights, lambda_=lambda_)\n","acc_ewcA, acc_ewcB = trn.train(model_ewcB,\n","                     epochs,\n","                     train_B,\n","                     fisher_matrix=f_matrix,\n","                     test_tasks=[test_A, test_B]\n","                    )\n","\n","print('[INFO] Task A ACC. after training B with EWC: {}'.format(acc_ewcA[-1]))\n","print('[INFO] Task B ACC. after training B with EWC: {}'.format(acc_ewcB[-1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nMC-EzBy7Xi"},"outputs":[],"source":["model_sgdB = mlp.get_compiled_model(opt, loss_fn, ['accuracy'])\n","model_sgdB.load_weights('CIFAR10_A.h5')\n","acc_sgdA, acc_sgdB = trn_gd.train(model_sgdB, epochs, train_B, test_tasks = [test_A, test_B])\n","\n","print('[INFO] Task A ACC. after training B with GD: {}'.format(acc_sgdA[-1]))\n","print('[INFO] Task B ACC. after training B with GD: {}'.format(acc_sgdB[-1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P1HDUclay9UU"},"outputs":[],"source":["x = 0\n","total_width, n = 0.1, 2\n","width = total_width / n\n","x = x - (total_width - width) / 2\n","plt.style.use('ggplot')\n","plt.bar(x, acc_ewcB[-1], width=width, label='EWC B', hatch='w/', ec='w')\n","plt.bar(x + width, acc_sgdB[-1], width=width, label='SGD B', hatch='w/', ec='w')\n","plt.bar(x + 3.5 * width, acc_prior_A[-1], width=width, label='Prior A', hatch='w/', ec='w')\n","plt.bar(x + 4.5 * width, acc_ewcA[-1], width=width, label='EWC A', hatch='w/', ec='w')\n","plt.bar(x + 5.5 * width, acc_sgdA[-1], width=width, label='SGD A', hatch='w/', ec='w')\n","plt.legend(facecolor='white', loc='lower left')\n","plt.xticks(np.array([0., 3.5 * width]), ('Task B', 'Task A'))\n","plt.title('Training task B with EWC Vs SGD after \\n task A had been trained to criterion')\n","plt.xlim(-0.15, 0.35)\n","plt.ylim(0., 105.)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s9K4IQRxzAwe"},"outputs":[],"source":["plt.plot(range(0, epochs*2, 1), (acc_prior_A + acc_sgdA), color='green', linestyle='dashed', label = \"SGD\")\n","plt.plot(range(0, epochs*2, 1), (acc_prior_A + acc_ewcA), color='red', linestyle='dashed', label = \"EWC\")\n","plt.plot(range(0, epochs, 1), (acc_prior_A), color='blue', label = \"Prior\")\n","#plt.axvline(x=9, linestyle='dashed', color='green')\n","plt.xticks(range(0, epochs*2, 50))\n","plt.title('Training task B with EWC Vs SGD after \\n task A had been trained to criterion')\n","plt.legend(facecolor='white')\n","plt.ylabel('Test accuracy A')\n","plt.xlabel('Epochs')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyOQr6yUls6UgO+9U64jAWKq"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}